{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility for mobilization: Cohort identification. \n",
    "\n",
    "This script identifies the cohort using CLIF 2.0 tables. \n",
    "\n",
    "Requirements:\n",
    "* Required table filenames should be clif_patient, clif_hospitalization, clif_adt, clif_vitals, clif_labs, clif_medication_admin_continuous, clif_respiratory_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from config.json\n",
      "{'site_name': 'UCMC', 'tables_path': '/Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19', 'file_type': 'parquet'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import pyCLIF\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set'\n",
    "]\n",
    "\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'resp_rate', 'sbp', 'dbp', 'map', 'resp_rate', 'spo2']\n",
    "\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value'\n",
    "]\n",
    "labs_of_interest = ['lactate']\n",
    "\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'nicardipine', 'nitroprusside',\n",
    "    'clevidipine', 'cisatracurium'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_patient.parquet\n",
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_hospitalization.parquet\n"
     ]
    }
   ],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.standardize_datetime(patient)\n",
    "hospitalization = pyCLIF.standardize_datetime(hospitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: patient\n",
      "No duplicates found based on columns: ['patient_id'].\n",
      "Processing DataFrame: hospitalization\n",
      "No duplicates found based on columns: ['hospitalization_id'].\n"
     ]
    }
   ],
   "source": [
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of unique encounters in the data: 448402\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of unique encounters in the data: {pyCLIF.count_unique_encounters(hospitalization, 'hospitalization_id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion Criteria:\n",
    "\n",
    "* Filter Admissions for March 1, 2020 - March 31, 2022\n",
    "* Encounters receiving invasive mechanical ventilation during this period\n",
    "\n",
    "### Exclusion criteria:\n",
    "\n",
    "1. Encounters that were on vent for less than 2 hours\n",
    "2. Encounters that were on trach in the first 72 hours \n",
    "3. Encounters that received Cisatracurium for 4 hours or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique encounters after filtering by date and age: 164613\n"
     ]
    }
   ],
   "source": [
    "cohort = hospitalization[\n",
    "    (hospitalization['admission_dttm'] >= '2020-03-01') &\n",
    "    (hospitalization['admission_dttm'] <= '2022-03-31') &\n",
    "    (hospitalization['age_at_admission'] >= 18)\n",
    "].reset_index(drop=True)[['hospitalization_id']].drop_duplicates()\n",
    "\n",
    "cohort_ids = cohort['hospitalization_id'].unique().tolist()\n",
    "print(f\"Number of unique encounters after filtering by date and age:\", cohort['hospitalization_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_respiratory_support.parquet\n"
     ]
    }
   ],
   "source": [
    "# Import clif respiratory table for this cohort\n",
    "rst_filters = {\n",
    "    'hospitalization_id': cohort_ids\n",
    "}\n",
    "resp_support_raw = pyCLIF.load_data('clif_respiratory_support', columns=rst_required_columns, filters=rst_filters)\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating waterfall processing...\n",
      "Fixing out-of-range values for 'fio2_set', 'peep_set', and 'resp_rate_set'...\n",
      "Creating recorded_date and recorded_hour...\n",
      "Sorting data by 'hospitalization_id' and 'recorded_dttm'...\n",
      "Fixing missing 'device_category' and 'device_name' based on 'mode_category'...\n",
      "Fixing 'device_category' and 'device_name' based on neighboring records...\n",
      "Handling duplicates and removing rows with all key variables missing...\n",
      "Filling forward 'device_category' within each hospitalization...\n",
      "Creating 'device_cat_id' to track changes in 'device_category'...\n",
      "Filling 'device_name' within each 'device_cat_id'...\n",
      "Creating 'device_id' to track changes in 'device_name'...\n",
      "Filling 'mode_category' within each 'device_id'...\n",
      "Creating 'mode_cat_id' to track changes in 'mode_category'...\n",
      "Filling 'mode_name' within each 'mode_cat_id'...\n",
      "Creating 'mode_name_id' to track changes in 'mode_name'...\n",
      "Adjusting 'fio2_set' for 'room air' device_category...\n",
      "Adjusting 'mode_category' for 't-piece' devices...\n",
      "Filling remaining variables within each 'mode_name_id'...\n",
      "Filling 'tracheostomy' forward within each hospitalization...\n",
      "Removing duplicates...\n",
      "Waterfall processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Apply Nick's Waterfall fill logic for respiratory support table\n",
    "processed_resp_support = pyCLIF.process_resp_support(resp_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique encounters after filtering for ventilator usage: 5690\n"
     ]
    }
   ],
   "source": [
    "# Identify the cohort on invasive mechanical ventilation \n",
    "columns_to_keep = [\n",
    "    'hospitalization_id', 'recorded_dttm', 'device_name','device_category',\n",
    "    'mode_name', 'mode_category' , 'tracheostomy',\n",
    "    'fio2_set', 'lpm_set', 'peep_set', \n",
    "    'resp_rate_obs', 'resp_rate_set'\n",
    "]\n",
    "\n",
    "ventilator_usage = processed_resp_support[processed_resp_support['device_category'].str.contains(\"imv\", case=False, na=False)]\n",
    "cohort_on_vent = ventilator_usage.merge(cohort, on='hospitalization_id', how='left')\n",
    "cohort_ids = cohort_on_vent['hospitalization_id'].unique().tolist()\n",
    "cohort_on_vent.loc[:, 'recorded_dttm'] = pd.to_datetime(cohort_on_vent['recorded_dttm'])\n",
    "\n",
    "cohort_on_vent = cohort_on_vent[columns_to_keep]\n",
    "cohort_on_vent['on_vent'] = cohort_on_vent['device_category'].str.contains(\"imv\", case=False, na=False).astype(int)\n",
    "cohort_on_vent = cohort_on_vent.sort_values(by=['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "vent_start_end = cohort_on_vent[cohort_on_vent['on_vent'] == 1].groupby('hospitalization_id').agg(\n",
    "    vent_start_time=('recorded_dttm', 'min'),\n",
    "    vent_end_time=('recorded_dttm', 'max'),\n",
    "    recorded_dttm=('recorded_dttm', lambda x: x)\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Number of unique encounters after filtering for ventilator usage: {cohort_on_vent['hospitalization_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_medication_admin_continuous.parquet\n",
      "unique encounters in meds 4038\n"
     ]
    }
   ],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)\n",
    "print(\"unique encounters in meds\", pyCLIF.count_unique_encounters(meds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_labs.parquet\n",
      "unique encounters in labs 5529\n"
     ]
    }
   ],
   "source": [
    "# Import clif continuous meds and clif labs table for the cohort on vent during the required time period\n",
    "labs_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /Users/kavenchhikara/Desktop/CLIF/CLIF-UCMC/rclif/c19/clif_vitals.parquet\n"
     ]
    }
   ],
   "source": [
    "vitals_filters = {\n",
    "    'hospitalization_id': cohort_ids,\n",
    "    'vital_category': vitals_of_interest\n",
    "}\n",
    "vitals = pyCLIF.load_data('clif_vitals', columns=vitals_required_columns, filters=vitals_filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters in vitals 5666\n"
     ]
    }
   ],
   "source": [
    "# Get first_vital_dttm and last_vital_dttm for each hospitalization_id in one step\n",
    "vital_dttm_bounds = vitals.groupby('hospitalization_id')['recorded_dttm'].agg(['min', 'max']).reset_index()\n",
    "vital_dttm_bounds.columns = ['hospitalization_id', 'first_vital_dttm', 'last_vital_dttm']\n",
    "print(\"unique encounters in vitals\", pyCLIF.count_unique_encounters(vital_dttm_bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly sequence for the cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters in resp filtered 5666\n"
     ]
    }
   ],
   "source": [
    "final_cohort = vent_start_end.merge(vital_dttm_bounds, on='hospitalization_id', how='inner')\n",
    "print(\"unique encounters in resp filtered\", pyCLIF.count_unique_encounters(final_cohort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/jxkvz04s3q55f82vwjbj_5w80000gq/T/ipykernel_19845/352824625.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hour_sequence = final_cohort.groupby('hospitalization_id')\\\n"
     ]
    }
   ],
   "source": [
    "# Function to generate hourly sequence for each group (hospitalization_id)\n",
    "def generate_hourly_sequence(group):\n",
    "    # Get the vent start time and discharge time\n",
    "    start_time = group['vent_start_time'].iloc[0]\n",
    "    last_vital_time = group['last_vital_dttm'].iloc[0]\n",
    "    \n",
    "    # Check if the last_vital_time is before the vent start time\n",
    "    if last_vital_time < start_time:\n",
    "        end_time = group['recorded_dttm'].max()  # Use the max recorded_dttm in this case\n",
    "    else:\n",
    "        end_time = last_vital_time  # Otherwise, use the last_vital_time\n",
    "    \n",
    "    # Generate the sequence of hourly timestamps\n",
    "    hourly_timestamps = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "    \n",
    "    # Create a new DataFrame for this sequence\n",
    "    return pd.DataFrame({\n",
    "        'hospitalization_id': group['hospitalization_id'].iloc[0],\n",
    "        'recorded_dttm': hourly_timestamps\n",
    "    })\n",
    "\n",
    "# Apply the function to each group and concatenate the results, using include_groups=False\n",
    "# hour_sequence = final_cohort.groupby('hospitalization_id').apply(generate_hourly_sequence).reset_index(drop=True)\n",
    "hour_sequence = final_cohort.groupby('hospitalization_id')\\\n",
    "    .apply(generate_hourly_sequence)\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# Add `recorded_date` and `recorded_hour` columns\n",
    "# Convert recorded_dttm to datetime if not already done\n",
    "hour_sequence['recorded_dttm'] = pd.to_datetime(hour_sequence['recorded_dttm'])\n",
    "hour_sequence['recorded_date'] = hour_sequence['recorded_dttm'].dt.date\n",
    "hour_sequence['recorded_hour'] = hour_sequence['recorded_dttm'].dt.hour\n",
    "hour_sequence['time_from_vent'] = hour_sequence.groupby('hospitalization_id').cumcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Respiratory support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_filtered = cohort_on_vent[columns_to_keep]\n",
    "# Apply thresholds and replace values outside these with NaN using .loc[]\n",
    "cohort_on_vent.loc[:, 'fio2_set'] = cohort_on_vent['fio2_set'].where(cohort_on_vent['fio2_set'].between(0.21, 1, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_set'] = cohort_on_vent['resp_rate_set'].where(cohort_on_vent['resp_rate_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'peep_set'] = cohort_on_vent['peep_set'].where(cohort_on_vent['peep_set'].between(0, 50, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'resp_rate_obs'] = cohort_on_vent['resp_rate_obs'].where(cohort_on_vent['resp_rate_obs'].between(0, 100, inclusive='both'), np.nan)\n",
    "cohort_on_vent.loc[:, 'lpm_set'] = cohort_on_vent['lpm_set'].where(cohort_on_vent['lpm_set'].between(0, 60, inclusive='both'), np.nan)\n",
    "\n",
    "cohort_on_vent['recorded_date'] = cohort_on_vent['recorded_dttm'].dt.date\n",
    "cohort_on_vent['recorded_hour'] = cohort_on_vent['recorded_dttm'].dt.hour\n",
    "hourly_vent_df = cohort_on_vent.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    min_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='min'),\n",
    "    min_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='min'),\n",
    "    min_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='min'),\n",
    "    min_peep_set=pd.NamedAgg(column='peep_set', aggfunc='min'),\n",
    "    max_resp_rate_obs=pd.NamedAgg(column='resp_rate_obs', aggfunc='max'),\n",
    "    max_lpm_set=pd.NamedAgg(column='lpm_set', aggfunc='max'),\n",
    "    max_fio2_set=pd.NamedAgg(column='fio2_set', aggfunc='max'),\n",
    "    max_peep_set=pd.NamedAgg(column='peep_set', aggfunc='max'),\n",
    "    hourly_trach=pd.NamedAgg(column='tracheostomy', aggfunc=lambda x: 1 if x.max() == 1 else 0),\n",
    "    hourly_on_vent=pd.NamedAgg(column='on_vent', aggfunc=lambda x: 1 if x.max() == 1 else 0)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique encounters in merged df 5666\n"
     ]
    }
   ],
   "source": [
    "# Merge hourly_vent_df with hour_sequence on hospitalization_id, recorded_date, and recorded_hour\n",
    "final_df = pd.merge(hour_sequence, hourly_vent_df, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                     how='left')\n",
    "print(\"unique encounters in merged df\", pyCLIF.count_unique_encounters(final_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals['recorded_dttm'] = pd.to_datetime(vitals['recorded_dttm'])\n",
    "vitals['recorded_hour'] = vitals['recorded_dttm'].dt.hour\n",
    "vitals['recorded_date'] = vitals['recorded_dttm'].dt.date\n",
    "\n",
    "vitals_min_max = vitals.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour', 'vital_category']).agg(\n",
    "    min=pd.NamedAgg(column='vital_value', aggfunc='min'),\n",
    "    max=pd.NamedAgg(column='vital_value', aggfunc='max')\n",
    ").reset_index()\n",
    "\n",
    "# Pivot the table to reshape it\n",
    "vitals_pivot = vitals_min_max.pivot_table(\n",
    "    index=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    columns='vital_category',\n",
    "    values=['min', 'max']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the column multi-index after pivot\n",
    "vitals_pivot.columns = ['_'.join(col).strip() if type(col) is tuple else col for col in vitals_pivot.columns]\n",
    "# Remove trailing underscores\n",
    "vitals_pivot.columns = [col.rstrip('_') for col in vitals_pivot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hospitalization_id', 'recorded_date', 'recorded_hour', 'max_dbp',\n",
      "       'max_heart_rate', 'max_map', 'max_sbp', 'max_spo2', 'min_dbp',\n",
      "       'min_heart_rate', 'min_map', 'min_sbp', 'min_spo2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vitals_pivot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vitals with the blocked resp support data\n",
    "final_df = pd.merge(final_df, vitals_pivot, on=['hospitalization_id', 'recorded_date', 'recorded_hour'], \n",
    "                   how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospitalization_id', 'recorded_dttm', 'recorded_date', 'recorded_hour',\n",
       "       'time_from_vent', 'min_resp_rate_obs', 'min_lpm_set', 'min_fio2_set',\n",
       "       'min_peep_set', 'max_resp_rate_obs', 'max_lpm_set', 'max_fio2_set',\n",
       "       'max_peep_set', 'hourly_trach', 'hourly_on_vent', 'max_dbp',\n",
       "       'max_heart_rate', 'max_map', 'max_sbp', 'max_spo2', 'min_dbp',\n",
       "       'min_heart_rate', 'min_map', 'min_sbp', 'min_spo2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobilization)",
   "language": "python",
   "name": ".mobilization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
